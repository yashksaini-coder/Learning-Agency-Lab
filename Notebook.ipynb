{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'><img src='./header.png'></p>\n",
    "\n",
    "# <p align='center'> <img src='./LabLogo_Kaggle.png' height='30rem' width='30rem'> Learning Agency Lab - Automated Essay Scoring 2.0</p>\n",
    "\n",
    "**Learning Agency Lab - Automated Essay Scoring 2.0:-** The goal is to build a model that can accurately predict the score an essay deserves based solely on its text content. The competition aims to improve student learning outcomes by providing timely and reliable feedback to overburdened educators.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Essay writing is a crucial method to evaluate student learning and performance, but it is time-consuming for educators to grade manually.<br>\n",
    "- **Automated Writing Evaluation (AWE)** systems can assist in scoring essays, providing students with regular and timely feedback. However, many advancements in AWE are not widely accessible due to cost barriers. Open-source solutions are needed to make AWE technology available to every community, especially underserved ones.\n",
    "\n",
    "## Competition Objective\n",
    "\n",
    "The objective of this competition is to train a model to score student essays accurately. Participants are tasked with reducing the high expense and time required for manual grading, making it feasible to introduce essays into testing, a key indicator of student learning.\n",
    "\n",
    "## Dataset\n",
    "The competition dataset comprises about 24000 student-written argumentative essays. Each essay was scored on a scale of 1 to 6 (Link to the Holistic Scoring Rubric). Your goal is to predict the score an essay received from its text.\n",
    "\n",
    "***File and Field Information:-***\n",
    "Sure, here's the information organized in a tabular form:\n",
    "\n",
    "| File Name          | Description                                             | Fields                                  |\n",
    "|--------------------|---------------------------------------------------------|-----------------------------------------|\n",
    "| train.csv          | Essays and scores to be used as training data           | essay_id, full_text, score              |\n",
    "| test.csv           | Essays to be used as test data                          | essay_id, full_text                     |\n",
    "| sample_submission.csv | A submission file in the correct format                | essay_id, score                        |\n",
    "\n",
    "Each file contains specific fields:\n",
    "\n",
    "- `train.csv`: Contains essays along with their unique ID (`essay_id`), the full text of the essay (`full_text`), and the holistic score of the essay on a 1-6 scale (`score`).\n",
    "- `test.csv`: Contains essays to be used as test data, including their unique ID (`essay_id`) and the full text of the essay (`full_text`). This file does not include the `score` field.\n",
    "- `sample_submission.csv`: A submission file template with the correct format for submission. It includes the unique ID of each essay (`essay_id`) and a placeholder for the predicted holistic score of the essay on a 1-6 scale (`score`).\n",
    "\n",
    "This tabular representation summarizes the contents of each file and their respective fields, providing clarity on the dataset structure and file formats.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two outcomes. This metric typically varies from 0 (random agreement) to 1 (complete agreement). In the event that there is less agreement than expected by chance, the metric may go below 0.\n",
    "\n",
    "The quadratic weighted kappa is calculated as follows. First, an N x N histogram matrix O is constructed, such that Oi,j corresponds to the number of essay_ids i (actual) that received a predicted value j. An N-by-N matrix of weights, w, is calculated based on the difference between actual and predicted values:\n",
    "\n",
    "<div align='center'>\n",
    "<span class=\"MathJax\" id=\"MathJax-Element-1-Frame\" tabindex=\"0\" data-mathml=\"<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><msub><mi>w</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub><mo>=</mo><mfrac><msup><mrow><mo>(</mo><mi>i</mi><mo>&amp;#x2212;</mo><mi>j</mi><mo>)</mo></mrow><mn>2</mn></msup><msup><mrow><mo>(</mo><mi>N</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo>)</mo></mrow><mn>2</mn></msup></mfrac></math>\" role=\"presentation\" style=\"text-align: center; position: relative;\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-1\" style=\"width: 8.96em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.089em; height: 0px; font-size: 126%;\"><span style=\"position: absolute; clip: rect(0.4em, 1007.09em, 3.631em, -999.997em); top: -2.265em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-2\"><span class=\"msubsup\" id=\"MathJax-Span-3\"><span style=\"display: inline-block; position: relative; width: 1.533em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.404em, 1000.68em, 4.198em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-4\" style=\"font-family: MathJax_Math-italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.74em;\"><span class=\"texatom\" id=\"MathJax-Span-5\"><span class=\"mrow\" id=\"MathJax-Span-6\"><span class=\"mi\" id=\"MathJax-Span-7\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-8\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-9\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-10\" style=\"font-family: MathJax_Main; padding-left: 0.286em;\">=</span><span class=\"mfrac\" id=\"MathJax-Span-11\" style=\"padding-left: 0.286em;\"><span style=\"display: inline-block; position: relative; width: 3.971em; height: 0px; margin-right: 0.116em; margin-left: 0.116em;\"><span style=\"position: absolute; clip: rect(2.894em, 1003.18em, 4.425em, -999.997em); top: -4.759em; left: 50%; margin-left: -1.584em;\"><span class=\"msubsup\" id=\"MathJax-Span-12\"><span style=\"display: inline-block; position: relative; width: 3.177em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.121em, 1002.67em, 4.425em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-13\"><span class=\"mo\" id=\"MathJax-Span-14\" style=\"font-family: MathJax_Main;\">(</span><span class=\"mi\" id=\"MathJax-Span-15\" style=\"font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-16\" style=\"font-family: MathJax_Main; padding-left: 0.23em;\">−</span><span class=\"mi\" id=\"MathJax-Span-17\" style=\"font-family: MathJax_Math-italic; padding-left: 0.23em;\">j</span><span class=\"mo\" id=\"MathJax-Span-18\" style=\"font-family: MathJax_Main;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -4.476em; left: 2.781em;\"><span class=\"mn\" id=\"MathJax-Span-19\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; clip: rect(2.894em, 1003.86em, 4.425em, -999.997em); top: -3.115em; left: 50%; margin-left: -1.925em;\"><span class=\"msubsup\" id=\"MathJax-Span-20\"><span style=\"display: inline-block; position: relative; width: 3.858em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.121em, 1003.29em, 4.425em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-21\"><span class=\"mo\" id=\"MathJax-Span-22\" style=\"font-family: MathJax_Main;\">(</span><span class=\"mi\" id=\"MathJax-Span-23\" style=\"font-family: MathJax_Math-italic;\">N<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.06em;\"></span></span><span class=\"mo\" id=\"MathJax-Span-24\" style=\"font-family: MathJax_Main; padding-left: 0.23em;\">−</span><span class=\"mn\" id=\"MathJax-Span-25\" style=\"font-family: MathJax_Main; padding-left: 0.23em;\">1</span><span class=\"mo\" id=\"MathJax-Span-26\" style=\"font-family: MathJax_Main;\">)</span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -4.476em; left: 3.404em;\"><span class=\"mn\" id=\"MathJax-Span-27\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">2</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; clip: rect(0.853em, 1003.97em, 1.25em, -999.997em); top: -1.301em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 3.971em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.08em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 2.27em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.568em; border-left: 0px solid; width: 0px; height: 3.718em;\"></span></span></nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "An N-by-N histogram matrix of expected outcomes, E, is calculated assuming that there is no correlation between values. \n",
    "This is calculated as the outer product between the actual histogram vector of outcomes and the predicted histogram vector, normalized such that E and O have the same sum.\n",
    "\n",
    "From these three matrices, the quadratic weighted kappa is calculated as: \n",
    "\n",
    "<div align=\"center\">\n",
    "<span class=\"math\" id=\"MathJax-Span-138\" style=\"width: 11.964em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 9.47em; height: 0px; font-size: 126%;\"><span style=\"position: absolute; clip: rect(0.456em, 1009.41em, 3.574em, -999.997em); top: -2.265em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-139\"><span class=\"mi\" id=\"MathJax-Span-140\" style=\"font-family: MathJax_Math-italic;\">κ</span><span class=\"mo\" id=\"MathJax-Span-141\" style=\"font-family: MathJax_Main; padding-left: 0.286em;\">=</span><span class=\"mn\" id=\"MathJax-Span-142\" style=\"font-family: MathJax_Main; padding-left: 0.286em;\">1</span><span class=\"mo\" id=\"MathJax-Span-143\" style=\"font-family: MathJax_Main; padding-left: 0.23em;\">−</span><span class=\"mfrac\" id=\"MathJax-Span-144\" style=\"padding-left: 0.23em;\"><span style=\"display: inline-block; position: relative; width: 5.275em; height: 0px; margin-right: 0.116em; margin-left: 0.116em;\"><span style=\"position: absolute; clip: rect(3.121em, 1005.11em, 4.651em, -999.997em); top: -4.929em; left: 50%; margin-left: -2.548em;\"><span class=\"mrow\" id=\"MathJax-Span-145\"><span class=\"munderover\" id=\"MathJax-Span-146\"><span style=\"display: inline-block; position: relative; width: 1.874em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.121em, 1001.02em, 4.425em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-147\" style=\"font-family: MathJax_Size1; vertical-align: 0em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.739em; left: 1.08em;\"><span class=\"texatom\" id=\"MathJax-Span-148\"><span class=\"mrow\" id=\"MathJax-Span-149\"><span class=\"mi\" id=\"MathJax-Span-150\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-151\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-152\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-153\" style=\"padding-left: 0.173em;\"><span style=\"display: inline-block; position: relative; width: 1.533em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.404em, 1000.68em, 4.198em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-154\" style=\"font-family: MathJax_Math-italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.74em;\"><span class=\"texatom\" id=\"MathJax-Span-155\"><span class=\"mrow\" id=\"MathJax-Span-156\"><span class=\"mi\" id=\"MathJax-Span-157\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-158\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-159\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-160\"><span style=\"display: inline-block; position: relative; width: 1.59em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.177em, 1000.74em, 4.198em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-161\" style=\"font-family: MathJax_Math-italic;\">O</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.74em;\"><span class=\"texatom\" id=\"MathJax-Span-162\"><span class=\"mrow\" id=\"MathJax-Span-163\"><span class=\"mi\" id=\"MathJax-Span-164\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-165\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-166\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; clip: rect(3.121em, 1005.11em, 4.651em, -999.997em); top: -3.285em; left: 50%; margin-left: -2.548em;\"><span class=\"mrow\" id=\"MathJax-Span-167\"><span class=\"munderover\" id=\"MathJax-Span-168\"><span style=\"display: inline-block; position: relative; width: 1.874em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.121em, 1001.02em, 4.425em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-169\" style=\"font-family: MathJax_Size1; vertical-align: 0em;\">∑</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.739em; left: 1.08em;\"><span class=\"texatom\" id=\"MathJax-Span-170\"><span class=\"mrow\" id=\"MathJax-Span-171\"><span class=\"mi\" id=\"MathJax-Span-172\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-173\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-174\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-175\" style=\"padding-left: 0.173em;\"><span style=\"display: inline-block; position: relative; width: 1.533em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.404em, 1000.68em, 4.198em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-176\" style=\"font-family: MathJax_Math-italic;\">w</span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.74em;\"><span class=\"texatom\" id=\"MathJax-Span-177\"><span class=\"mrow\" id=\"MathJax-Span-178\"><span class=\"mi\" id=\"MathJax-Span-179\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-180\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-181\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span><span class=\"msubsup\" id=\"MathJax-Span-182\"><span style=\"display: inline-block; position: relative; width: 1.533em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.177em, 1000.74em, 4.198em, -999.997em); top: -4.022em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-183\" style=\"font-family: MathJax_Math-italic;\">E<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; top: -3.852em; left: 0.74em;\"><span class=\"texatom\" id=\"MathJax-Span-184\"><span class=\"mrow\" id=\"MathJax-Span-185\"><span class=\"mi\" id=\"MathJax-Span-186\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">i</span><span class=\"mo\" id=\"MathJax-Span-187\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,</span><span class=\"mi\" id=\"MathJax-Span-188\" style=\"font-size: 70.7%; font-family: MathJax_Math-italic;\">j</span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span></span></span></span><span style=\"display: inline-block; width: 0px; height: 4.028em;\"></span></span><span style=\"position: absolute; clip: rect(0.853em, 1005.27em, 1.25em, -999.997em); top: -1.301em; left: 0em;\"><span style=\"display: inline-block; overflow: hidden; vertical-align: 0em; border-top: 1.3px solid; width: 5.275em; height: 0px;\"></span><span style=\"display: inline-block; width: 0px; height: 1.08em;\"></span></span></span></span><span class=\"mo\" id=\"MathJax-Span-189\" style=\"font-family: MathJax_Main;\">.</span></span><span style=\"display: inline-block; width: 0px; height: 2.27em;\"></span></span></span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.496em; border-left: 0px solid; width: 0px; height: 3.718em;\"></span></span>\n",
    "</div>\n",
    "\n",
    "## Submission File\n",
    "\n",
    "For each essay_id in the test set, participants must predict the corresponding score. The submission file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "essay_id,score\n",
    "000d118,3\n",
    "000fe60,3\n",
    "001ab80,4\n",
    "```\n",
    "\n",
    "---\n",
    "For detailed instructions, guidelines, and access to the dataset, please visit the competition page on Kaggle: [Learning Agency Lab - Automated Essay Scoring 2.0](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"/kaggle/input/pyspellchecker/pyspellchecker-0.7.2-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import string\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load dataset and initial configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PATHS:\n",
    "    train_path = 'data/train.csv'\n",
    "    test_path = 'data/test.csv'\n",
    "    sub_path = 'data/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    n_splits = 5\n",
    "    seed = 42\n",
    "    num_labels = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score\n",
       "0  000d118  Many people have car where they live. The thin...      3\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       "2  001ab80  People always wish they had the same technolog...      4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(PATHS.train_path)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text\n",
       "0  000d118  Many people have car where they live. The thin...\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...\n",
       "2  001ab80  People always wish they had the same technolog..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(PATHS.test_path)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data preprocessing functions definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHTML(x):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',x)\n",
    "\n",
    "\n",
    "cList = {\n",
    "    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\",  ## --> he had or he would\n",
    "    \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \n",
    "    \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",   ## --> I had or I would\n",
    "    \"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",   ## --> It had or It would\n",
    "    \"it'd've\": \"it would have\",\"it'll\": \"it will\",\"it'll've\": \"it will have\",\"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",   ## --> It had or It would\n",
    "    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there'd've\": \"there would have\",\"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\"wasn't\": \"was not\",\"weren't\": \"were not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\"wouldn't\": \"would not\",\"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\n",
    "    \"you're\": \"you are\",  \"you've\": \"you have\"\n",
    "}\n",
    "c_re = re.compile('(%s)' % '|'.join(cList.keys()))\n",
    "\n",
    "def expandContractions(text):\n",
    "    def replace(match):\n",
    "        return cList[match.group(0)]\n",
    "    return c_re.sub(replace, text)\n",
    "\n",
    "def dataPreprocessing(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Remove \\xa0\n",
    "    x = x.replace(u'\\xa0',' ')\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # string.punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "def dataPreprocessing_w_contract_punct_remove(x):\n",
    "    # Convert words to lowercase\n",
    "    x = x.lower()\n",
    "    # Remove HTML\n",
    "    x = removeHTML(x)\n",
    "    # Delete strings starting with @\n",
    "    x = re.sub(\"@\\w+\", '',x)\n",
    "    # Delete Numbers\n",
    "    x = re.sub(\"'\\d+\", '',x)\n",
    "    x = re.sub(\"\\d+\", '',x)\n",
    "    # Delete URL\n",
    "    x = re.sub(\"http\\w+\", '',x)\n",
    "    # Replace consecutive empty spaces with a single space character\n",
    "    x = re.sub(r\"\\s+\", \" \", x)\n",
    "    x = expandContractions(x)\n",
    "    # Replace consecutive commas and periods with one comma and period character\n",
    "    x = re.sub(r\"\\.+\", \".\", x)\n",
    "    x = re.sub(r\"\\,+\", \",\", x)\n",
    "    x = remove_punctuation(x)\n",
    "    # Remove empty characters at the beginning and end\n",
    "    x = x.strip()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Paragraph based feature\n",
    "\n",
    "<a id='paragraph-feature'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: can be fixed by keeping \"\\n\" and removed empty paragraph entries\n",
    "columns = [(pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\"))]\n",
    "train = pl.from_pandas(train).with_columns(columns)\n",
    "test = pl.from_pandas(test).with_columns(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  43\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>paragraph_word_cnt_sum</th>\n",
       "      <th>paragraph_len_kurtosis</th>\n",
       "      <th>paragraph_sentence_cnt_kurtosis</th>\n",
       "      <th>paragraph_word_cnt_kurtosis</th>\n",
       "      <th>paragraph_len_q1</th>\n",
       "      <th>paragraph_sentence_cnt_q1</th>\n",
       "      <th>paragraph_word_cnt_q1</th>\n",
       "      <th>paragraph_len_q3</th>\n",
       "      <th>paragraph_sentence_cnt_q3</th>\n",
       "      <th>paragraph_word_cnt_q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2645.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>2645.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>494.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>335</td>\n",
       "      <td>-1.301431</td>\n",
       "      <td>-1.044379</td>\n",
       "      <td>-1.310957</td>\n",
       "      <td>237.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>550</td>\n",
       "      <td>-1.740076</td>\n",
       "      <td>-1.592593</td>\n",
       "      <td>-1.696723</td>\n",
       "      <td>576.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>927.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>448</td>\n",
       "      <td>-1.449534</td>\n",
       "      <td>-1.750000</td>\n",
       "      <td>-1.333444</td>\n",
       "      <td>367.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>806.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>373</td>\n",
       "      <td>-1.475737</td>\n",
       "      <td>-1.616116</td>\n",
       "      <td>-1.457252</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "3  001bdc0                 5                 5                  5   \n",
       "4  002ba53                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "3                  5                  4                  4                  4   \n",
       "4                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  paragraph_word_cnt_sum  \\\n",
       "0                  1                  1  ...                     494   \n",
       "1                  3                  3  ...                     335   \n",
       "2                  4                  4  ...                     550   \n",
       "3                  4                  4  ...                     448   \n",
       "4                  4                  4  ...                     373   \n",
       "\n",
       "   paragraph_len_kurtosis  paragraph_sentence_cnt_kurtosis  \\\n",
       "0                     NaN                              NaN   \n",
       "1               -1.301431                        -1.044379   \n",
       "2               -1.740076                        -1.592593   \n",
       "3               -1.449534                        -1.750000   \n",
       "4               -1.475737                        -1.616116   \n",
       "\n",
       "   paragraph_word_cnt_kurtosis  paragraph_len_q1  paragraph_sentence_cnt_q1  \\\n",
       "0                          NaN            2645.0                       14.0   \n",
       "1                    -1.310957             237.0                        4.0   \n",
       "2                    -1.696723             576.0                        5.0   \n",
       "3                    -1.333444             367.0                        2.0   \n",
       "4                    -1.457252              19.0                        1.0   \n",
       "\n",
       "   paragraph_word_cnt_q1  paragraph_len_q3  paragraph_sentence_cnt_q3  \\\n",
       "0                  494.0            2645.0                       14.0   \n",
       "1                   48.0             398.0                        5.0   \n",
       "2                  101.0             927.0                        8.0   \n",
       "3                   64.0             806.0                        8.0   \n",
       "4                    3.0             559.0                        5.0   \n",
       "\n",
       "   paragraph_word_cnt_q3  \n",
       "0                  494.0  \n",
       "1                   77.0  \n",
       "2                  165.0  \n",
       "3                  128.0  \n",
       "4                   93.0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paragraph features\n",
    "def Paragraph_Preprocess(tmp):\n",
    "    # Expand the paragraph list into several lines of data\n",
    "    tmp = tmp.explode('paragraph')\n",
    "    # Paragraph preprocessing\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n",
    "    # Calculate the length of each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n",
    "    # Calculate the number of sentences and words in each paragraph\n",
    "    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n",
    "                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "paragraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\n",
    "def Paragraph_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of paragraph lengths greater than and less than the i-value\n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ], \n",
    "        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]], \n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea],  \n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea],\n",
    "    ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Paragraph_Preprocess(train)\n",
    "train_feats = Paragraph_Eng(tmp)\n",
    "\n",
    "# Obtain feature names\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sentence based features\n",
    "\n",
    "source: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  68\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_len_last</th>\n",
       "      <th>sentence_word_cnt_last</th>\n",
       "      <th>sentence_len_sum</th>\n",
       "      <th>sentence_word_cnt_sum</th>\n",
       "      <th>sentence_len_kurtosis</th>\n",
       "      <th>sentence_word_cnt_kurtosis</th>\n",
       "      <th>sentence_len_q1</th>\n",
       "      <th>sentence_word_cnt_q1</th>\n",
       "      <th>sentence_len_q3</th>\n",
       "      <th>sentence_word_cnt_q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>2632</td>\n",
       "      <td>506</td>\n",
       "      <td>1.485673</td>\n",
       "      <td>2.062371</td>\n",
       "      <td>110.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>125</td>\n",
       "      <td>26</td>\n",
       "      <td>1649</td>\n",
       "      <td>351</td>\n",
       "      <td>1.085089</td>\n",
       "      <td>0.464288</td>\n",
       "      <td>53.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>3041</td>\n",
       "      <td>573</td>\n",
       "      <td>-0.423362</td>\n",
       "      <td>0.129704</td>\n",
       "      <td>90.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  sentence_len_last  \\\n",
       "0                  1                  1  ...                 47   \n",
       "1                  3                  3  ...                125   \n",
       "2                  4                  4  ...                 58   \n",
       "\n",
       "   sentence_word_cnt_last  sentence_len_sum  sentence_word_cnt_sum  \\\n",
       "0                      10              2632                    506   \n",
       "1                      26              1649                    351   \n",
       "2                      10              3041                    573   \n",
       "\n",
       "   sentence_len_kurtosis  sentence_word_cnt_kurtosis  sentence_len_q1  \\\n",
       "0               1.485673                    2.062371            110.0   \n",
       "1               1.085089                    0.464288             53.0   \n",
       "2              -0.423362                    0.129704             90.0   \n",
       "\n",
       "   sentence_word_cnt_q1  sentence_len_q3  sentence_word_cnt_q3  \n",
       "0                  21.0            225.0                  37.0  \n",
       "1                  13.0            125.0                  26.0  \n",
       "2                  17.0            151.0                  29.0  \n",
       "\n",
       "[3 rows x 69 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence feature\n",
    "def Sentence_Preprocess(tmp):\n",
    "    # Preprocess full_text and use periods to segment sentences in the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n",
    "    tmp = tmp.explode('sentence')\n",
    "    # Calculate the length of a sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n",
    "    # Filter out the portion of data with a sentence length greater than 15\n",
    "    tmp = tmp.filter(pl.col('sentence_len')>=15)\n",
    "    # Count the number of words in each sentence\n",
    "    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "sentence_fea = ['sentence_len','sentence_word_cnt']\n",
    "def Sentence_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of sentences with a length greater than i\n",
    "        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ], \n",
    "        # other\n",
    "        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n",
    "        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea], \n",
    "        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea], \n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Sentence_Preprocess(train)\n",
    "\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Word based feature\n",
    "\n",
    "source: https://www.kaggle.com/code/ye11725/tfidf-lgbm-baseline-with-code-comments/notebook#Features-engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  89\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>word_12_cnt</th>\n",
       "      <th>word_13_cnt</th>\n",
       "      <th>word_14_cnt</th>\n",
       "      <th>word_15_cnt</th>\n",
       "      <th>word_len_max</th>\n",
       "      <th>word_len_mean</th>\n",
       "      <th>word_len_std</th>\n",
       "      <th>word_len_q1</th>\n",
       "      <th>word_len_q2</th>\n",
       "      <th>word_len_q3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>4.356275</td>\n",
       "      <td>2.537066</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3.976119</td>\n",
       "      <td>2.069025</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>4.574545</td>\n",
       "      <td>2.604621</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  word_12_cnt  word_13_cnt  \\\n",
       "0                  1                  1  ...            6            6   \n",
       "1                  3                  3  ...            0            0   \n",
       "2                  4                  4  ...           14           10   \n",
       "\n",
       "   word_14_cnt  word_15_cnt  word_len_max  word_len_mean  word_len_std  \\\n",
       "0            5            2            25       4.356275      2.537066   \n",
       "1            0            0            11       3.976119      2.069025   \n",
       "2            5            2            15       4.574545      2.604621   \n",
       "\n",
       "   word_len_q1  word_len_q2  word_len_q3  \n",
       "0          3.0          4.0          5.0  \n",
       "1          2.0          4.0          5.0  \n",
       "2          3.0          4.0          5.0  \n",
       "\n",
       "[3 rows x 90 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word feature\n",
    "def Word_Preprocess(tmp):\n",
    "    # Preprocess full_text and use spaces to separate words from the text\n",
    "    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n",
    "    tmp = tmp.explode('word')\n",
    "    # Calculate the length of each word\n",
    "    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n",
    "    # Delete data with a word length of 0\n",
    "    tmp = tmp.filter(pl.col('word_len')!=0)\n",
    "    \n",
    "    return tmp\n",
    "\n",
    "# feature_eng\n",
    "def Word_Eng(train_tmp):\n",
    "    aggs = [\n",
    "        # Count the number of words with a length greater than i+1\n",
    "        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n",
    "        # other\n",
    "        pl.col('word_len').max().alias(f\"word_len_max\"),\n",
    "        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n",
    "        pl.col('word_len').std().alias(f\"word_len_std\"),\n",
    "        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n",
    "        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n",
    "        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n",
    "        ]\n",
    "    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n",
    "    df = df.to_pandas()\n",
    "    return df\n",
    "\n",
    "tmp = Word_Preprocess(train)\n",
    "\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Character TFIDF feature:\n",
    "\n",
    "For TFIDF vector generation we use TfidfVectorizer provided by [sickit-learn liberay](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "#### Terms:\n",
    "* **TF (Term frequency)**: Number of time a term occur in a document / Total number of term in the document.\n",
    "* **DF (Document frequency)**: Number of document where the term appear / Total number of document.\n",
    "* **IDF (Inverse Document Frequency)**: 1 / Document frequency\n",
    "***\n",
    "### TfidfVectorizer parameters:\n",
    "* **tokenizer**: Is set to `lambda x: x` which means the text will be passed as it is. \n",
    "* **preprocessor**: Is set to `lambda x: x` which means the text will be passed as it is.\n",
    "* **token_pattern**: Is not set to `None` means word will be taken as token as it is without any word-level processing.\n",
    "* **strip_accents**: Ts set to `unicode` which means include unicode characters during preprocessing step.\n",
    "* **analyzer**: Ts set to `word` which means the feature (terms or token) will be the words\n",
    "* **ngram_range**: ngram_range equal to `(1, 2)` which means unigrams and bigrams\n",
    "* **min_df**: Is equal to `0.05` means ignore terms that occur in less the 5% of documents.\n",
    "* **max_df**: Is equal to `0.95` means ignore terms that occur in more them 95% of documents.\n",
    "* **sublinear_tf**: Is equal to `True` means replace tf with 1 + log(tf)\n",
    "\n",
    "##### Note:\n",
    "* **tokenizer=lambda x: x**: \"`words are not tokenized from full-text? Tokenizer should only be overided by identity if text is already tokenized before. Perhaps vectorizer is receiving string (char sequence) instead of word sequence, so it behaves like a char ngram vectorizer`\" qouted from notebook [here](https://www.kaggle.com/code/guillaums/error-in-tfidf-vectorizer-in-baseline-nbs?scriptVersionId=175110986&cellId=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "['  D e' '  D o' '  D r' '  E' '  E a' '  E l' '  E u' '  E v' '  E x'\n",
      " '  F']\n",
      "################################################################################ \n",
      "\n",
      "\n",
      "Features Number:  3380\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_3281</th>\n",
       "      <th>tfid_3282</th>\n",
       "      <th>tfid_3283</th>\n",
       "      <th>tfid_3284</th>\n",
       "      <th>tfid_3285</th>\n",
       "      <th>tfid_3286</th>\n",
       "      <th>tfid_3287</th>\n",
       "      <th>tfid_3288</th>\n",
       "      <th>tfid_3289</th>\n",
       "      <th>tfid_3290</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034738</td>\n",
       "      <td>0.071064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  tfid_3281  tfid_3282  tfid_3283  \\\n",
       "0                  1                  1  ...        0.0        0.0        0.0   \n",
       "1                  3                  3  ...        0.0        0.0        0.0   \n",
       "2                  4                  4  ...        0.0        0.0        0.0   \n",
       "\n",
       "   tfid_3284  tfid_3285  tfid_3286  tfid_3287  tfid_3288  tfid_3289  tfid_3290  \n",
       "0        0.0        0.0        0.0   0.034738   0.071064        0.0        0.0  \n",
       "1        0.0        0.0        0.0   0.000000   0.000000        0.0        0.0  \n",
       "2        0.0        0.0        0.0   0.000000   0.000000        0.0        0.0  \n",
       "\n",
       "[3 rows x 3381 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer parameter\n",
    "vectorizer = TfidfVectorizer(\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            token_pattern=None,\n",
    "            strip_accents='unicode',\n",
    "            analyzer = 'word',\n",
    "            ngram_range=(1,3),\n",
    "            min_df=0.05,\n",
    "            max_df=0.95,\n",
    "            sublinear_tf=True,\n",
    ")\n",
    "# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n",
    "train_tfid = vectorizer.fit_transform([i for i in train['full_text']])\n",
    "\n",
    "print(\"#\"*80)\n",
    "vect_feat_names=vectorizer.get_feature_names_out()\n",
    "print(vect_feat_names[100:110])\n",
    "print(\"#\"*80, \"\\n\\n\")\n",
    "\n",
    "# Convert to array\n",
    "dense_matrix = train_tfid.toarray()\n",
    "\n",
    "# Convert to dataframe\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "\n",
    "# rename features\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  3894\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_w_504</th>\n",
       "      <th>tfid_w_505</th>\n",
       "      <th>tfid_w_506</th>\n",
       "      <th>tfid_w_507</th>\n",
       "      <th>tfid_w_508</th>\n",
       "      <th>tfid_w_509</th>\n",
       "      <th>tfid_w_510</th>\n",
       "      <th>tfid_w_511</th>\n",
       "      <th>tfid_w_512</th>\n",
       "      <th>tfid_w_513</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099808</td>\n",
       "      <td>0.08042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3895 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  tfid_w_504  tfid_w_505  \\\n",
       "0                  1                  1  ...    0.000000    0.062121   \n",
       "1                  3                  3  ...    0.181618    0.000000   \n",
       "2                  4                  4  ...    0.000000    0.000000   \n",
       "\n",
       "   tfid_w_506  tfid_w_507  tfid_w_508  tfid_w_509  tfid_w_510  tfid_w_511  \\\n",
       "0         0.0         0.0    0.000000         0.0    0.099808     0.08042   \n",
       "1         0.0         0.0    0.095463         0.0    0.000000     0.00000   \n",
       "2         0.0         0.0    0.000000         0.0    0.000000     0.00000   \n",
       "\n",
       "   tfid_w_512  tfid_w_513  \n",
       "0         0.0         0.0  \n",
       "1         0.0         0.0  \n",
       "2         0.0         0.0  \n",
       "\n",
       "[3 rows x 3895 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "# TfidfVectorizer parameter\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    strip_accents='ascii',\n",
    "    analyzer = 'word',\n",
    "    ngram_range=(1,1),\n",
    "    min_df=0.05,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    stop_words=stopwords_list,\n",
    ")\n",
    "# Fit all datasets into TfidfVector,this may cause leakage and overly optimistic CV scores\n",
    "processed_text = train.to_pandas()[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n",
    "train_tfid = word_vectorizer.fit_transform([i for i in processed_text])\n",
    "\n",
    "# Convert to array\n",
    "dense_matrix = train_tfid.toarray()\n",
    "# Convert to dataframe\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "# rename features\n",
    "tfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_w_columns\n",
    "df['essay_id'] = train_feats['essay_id']\n",
    "\n",
    "df.head()\n",
    "# Merge the newly generated feature data with the previously generated feature data\n",
    "train_feats = train_feats.merge(df, on='essay_id', how='left')\n",
    "\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n",
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='extra-feature'></a>\n",
    "## 3.7 Extra features:\n",
    "Reference: https://www.kaggle.com/code/tsunotsuno/updated-debertav3-lgbm-with-spell-autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self) -> None:\n",
    "        self.twd = TreebankWordDetokenizer()\n",
    "        self.STOP_WORDS = set(stopwords.words('english'))\n",
    "        self.spellchecker = SpellChecker()\n",
    "\n",
    "    def spelling(self, text):\n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.spellchecker.unknown(wordlist)))\n",
    "        return amount_miss\n",
    "    \n",
    "    def count_sym(self, text, sym):\n",
    "        sym_count = 0\n",
    "        for l in text:\n",
    "            if l == sym:\n",
    "                sym_count += 1\n",
    "        return sym_count\n",
    "\n",
    "    def run(self, data: pd.DataFrame, mode:str) -> pd.DataFrame:\n",
    "        \n",
    "        # preprocessing the text\n",
    "        data[\"processed_text\"] = data[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n",
    "        \n",
    "        # Text tokenization\n",
    "        data[\"text_tokens\"] = data[\"processed_text\"].apply(lambda x: word_tokenize(x))\n",
    "        \n",
    "        # essay length\n",
    "        data[\"text_length\"] = data[\"processed_text\"].apply(lambda x: len(x))\n",
    "        \n",
    "        # essay word count\n",
    "        data[\"word_count\"] = data[\"text_tokens\"].apply(lambda x: len(x))\n",
    "        \n",
    "        # essay unique word count\n",
    "        data[\"unique_word_count\"] = data[\"text_tokens\"].apply(lambda x: len(set(x)))\n",
    "        \n",
    "        # essay sentence count\n",
    "        data[\"sentence_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('.')))\n",
    "        \n",
    "        # essay paragraph count\n",
    "        data[\"paragraph_count\"] = data[\"full_text\"].apply(lambda x: len(x.split('\\n\\n')))\n",
    "        \n",
    "        # count misspelling\n",
    "        data[\"splling_err_num\"] = data[\"processed_text\"].apply(self.spelling)\n",
    "        print(\"Spelling mistake count done\")\n",
    "        \n",
    "        return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling mistake count done\n"
     ]
    }
   ],
   "source": [
    "preprocessor = Preprocessor()\n",
    "tmp = preprocessor.run(train.to_pandas(), mode=\"train\")\n",
    "train_feats = train_feats.merge(tmp, on='essay_id', how='left')\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Number:  3904\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>score</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>splling_err_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Many people have car where they live. The thi...</td>\n",
       "      <td>many people have car where they live the thing...</td>\n",
       "      <td>[many, people, have, car, where, they, live, t...</td>\n",
       "      <td>2607</td>\n",
       "      <td>492</td>\n",
       "      <td>222</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>[I am a scientist at NASA that is discussing t...</td>\n",
       "      <td>i am a scientist at nasa that is discussing th...</td>\n",
       "      <td>[i, am, a, scientist, at, nasa, that, is, disc...</td>\n",
       "      <td>1630</td>\n",
       "      <td>335</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>[People always wish they had the same technolo...</td>\n",
       "      <td>people always wish they had the same technolog...</td>\n",
       "      <td>[people, always, wish, they, had, the, same, t...</td>\n",
       "      <td>3009</td>\n",
       "      <td>550</td>\n",
       "      <td>226</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3906 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  score  \\\n",
       "0                  1                  1  ...      3   \n",
       "1                  3                  3  ...      3   \n",
       "2                  4                  4  ...      4   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0  [Many people have car where they live. The thi...   \n",
       "1  [I am a scientist at NASA that is discussing t...   \n",
       "2  [People always wish they had the same technolo...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  many people have car where they live the thing...   \n",
       "1  i am a scientist at nasa that is discussing th...   \n",
       "2  people always wish they had the same technolog...   \n",
       "\n",
       "                                         text_tokens  text_length  word_count  \\\n",
       "0  [many, people, have, car, where, they, live, t...         2607         492   \n",
       "1  [i, am, a, scientist, at, nasa, that, is, disc...         1630         335   \n",
       "2  [people, always, wish, they, had, the, same, t...         3009         550   \n",
       "\n",
       "   unique_word_count  sentence_count  paragraph_count  splling_err_num  \n",
       "0                222              14                1               23  \n",
       "1                145              20                5                7  \n",
       "2                226              25                4                7  \n",
       "\n",
       "[3 rows x 3906 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Features Number: ',len(feature_names))\n",
    "train_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Test dataset featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paragraph\n",
    "tmp = Paragraph_Preprocess(test)\n",
    "test_feats = Paragraph_Eng(tmp)\n",
    "\n",
    "# Sentence\n",
    "tmp = Sentence_Preprocess(test)\n",
    "test_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n",
    "\n",
    "# Word\n",
    "tmp = Word_Preprocess(test)\n",
    "test_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tfidf\n",
    "test_tfid = vectorizer.transform([i for i in test['full_text']])\n",
    "dense_matrix = test_tfid.toarray()\n",
    "df = pd.DataFrame(dense_matrix)\n",
    "tfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\n",
    "df.columns = tfid_columns\n",
    "df['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df, on='essay_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word Tfidf\n",
    "processed_text = test.to_pandas()[\"full_text\"].apply(lambda x: dataPreprocessing_w_contract_punct_remove(x))\n",
    "# train_w_tfid = word_vectorizer.fit_transform(train['full_text'])\n",
    "test_w_tfid = word_vectorizer.fit_transform([i for i in processed_text])\n",
    "dense_matrix = test_w_tfid.toarray()\n",
    "df_w = pd.DataFrame(dense_matrix)\n",
    "tfid_w_columns = [ f'tfid_w_{i}' for i in range(len(df_w.columns))]\n",
    "df_w.columns = tfid_w_columns\n",
    "df_w['essay_id'] = test_feats['essay_id']\n",
    "test_feats = test_feats.merge(df_w, on='essay_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling mistake count done\n"
     ]
    }
   ],
   "source": [
    "# Extra feature\n",
    "\n",
    "preprocessor2 = Preprocessor()\n",
    "tmp = preprocessor2.run(test.to_pandas(), mode=\"train\")\n",
    "test_feats = test_feats.merge(tmp, on='essay_id', how='left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>full_text</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>splling_err_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>[Many people have car where they live. The thi...</td>\n",
       "      <td>many people have car where they live the thing...</td>\n",
       "      <td>[many, people, have, car, where, they, live, t...</td>\n",
       "      <td>2607</td>\n",
       "      <td>492</td>\n",
       "      <td>222</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>[I am a scientist at NASA that is discussing t...</td>\n",
       "      <td>i am a scientist at nasa that is discussing th...</td>\n",
       "      <td>[i, am, a, scientist, at, nasa, that, is, disc...</td>\n",
       "      <td>1630</td>\n",
       "      <td>335</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>[People always wish they had the same technolo...</td>\n",
       "      <td>people always wish they had the same technolog...</td>\n",
       "      <td>[people, always, wish, they, had, the, same, t...</td>\n",
       "      <td>3009</td>\n",
       "      <td>550</td>\n",
       "      <td>226</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  \\\n",
       "0                  1                  1  ...   \n",
       "1                  3                  3  ...   \n",
       "2                  4                  4  ...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Many people have car where they live. The thin...   \n",
       "1  I am a scientist at NASA that is discussing th...   \n",
       "2  People always wish they had the same technolog...   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0  [Many people have car where they live. The thi...   \n",
       "1  [I am a scientist at NASA that is discussing t...   \n",
       "2  [People always wish they had the same technolo...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  many people have car where they live the thing...   \n",
       "1  i am a scientist at nasa that is discussing th...   \n",
       "2  people always wish they had the same technolog...   \n",
       "\n",
       "                                         text_tokens  text_length  word_count  \\\n",
       "0  [many, people, have, car, where, they, live, t...         2607         492   \n",
       "1  [i, am, a, scientist, at, nasa, that, is, disc...         1630         335   \n",
       "2  [people, always, wish, they, had, the, same, t...         3009         550   \n",
       "\n",
       "   unique_word_count  sentence_count  paragraph_count  splling_err_num  \n",
       "0                222              14                1               23  \n",
       "1                145              20                5                7  \n",
       "2                226              25                4                7  \n",
       "\n",
       "[3 rows x 3759 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features number:  3758\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>full_text</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>splling_err_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>[Many people have car where they live. The thi...</td>\n",
       "      <td>many people have car where they live the thing...</td>\n",
       "      <td>[many, people, have, car, where, they, live, t...</td>\n",
       "      <td>2607</td>\n",
       "      <td>492</td>\n",
       "      <td>222</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>[I am a scientist at NASA that is discussing t...</td>\n",
       "      <td>i am a scientist at nasa that is discussing th...</td>\n",
       "      <td>[i, am, a, scientist, at, nasa, that, is, disc...</td>\n",
       "      <td>1630</td>\n",
       "      <td>335</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>[People always wish they had the same technolo...</td>\n",
       "      <td>people always wish they had the same technolog...</td>\n",
       "      <td>[people, always, wish, they, had, the, same, t...</td>\n",
       "      <td>3009</td>\n",
       "      <td>550</td>\n",
       "      <td>226</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  \\\n",
       "0                  1                  1  ...   \n",
       "1                  3                  3  ...   \n",
       "2                  4                  4  ...   \n",
       "\n",
       "                                           full_text  \\\n",
       "0  Many people have car where they live. The thin...   \n",
       "1  I am a scientist at NASA that is discussing th...   \n",
       "2  People always wish they had the same technolog...   \n",
       "\n",
       "                                           paragraph  \\\n",
       "0  [Many people have car where they live. The thi...   \n",
       "1  [I am a scientist at NASA that is discussing t...   \n",
       "2  [People always wish they had the same technolo...   \n",
       "\n",
       "                                      processed_text  \\\n",
       "0  many people have car where they live the thing...   \n",
       "1  i am a scientist at nasa that is discussing th...   \n",
       "2  people always wish they had the same technolog...   \n",
       "\n",
       "                                         text_tokens  text_length  word_count  \\\n",
       "0  [many, people, have, car, where, they, live, t...         2607         492   \n",
       "1  [i, am, a, scientist, at, nasa, that, is, disc...         1630         335   \n",
       "2  [people, always, wish, they, had, the, same, t...         3009         550   \n",
       "\n",
       "   unique_word_count  sentence_count  paragraph_count  splling_err_num  \n",
       "0                222              14                1               23  \n",
       "1                145              20                5                7  \n",
       "2                226              25                4                7  \n",
       "\n",
       "[3 rows x 3759 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Features number\n",
    "feature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\n",
    "print('Features number: ',len(feature_names))\n",
    "test_feats.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Add k-fold details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17307, 3907)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n",
    "for i, (_, val_index) in enumerate(skf.split(train_feats, train_feats[\"score\"])):\n",
    "    train_feats.loc[val_index, \"fold\"] = i\n",
    "print(train_feats.shape)\n",
    "# train_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3759)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"score\"\n",
    "train_drop_columns = [\"essay_id\", \"fold\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\",'tfid_w_368', 'tfid_w_369', 'tfid_w_370', 'tfid_w_371', 'tfid_w_372', 'tfid_w_373', 'tfid_w_374', 'tfid_w_375', 'tfid_w_376', 'tfid_w_377', 'tfid_w_378', 'tfid_w_379', 'tfid_w_380', 'tfid_w_381', 'tfid_w_382', 'tfid_w_383', 'tfid_w_384', 'tfid_w_385', 'tfid_w_386', 'tfid_w_387', 'tfid_w_388', 'tfid_w_389', 'tfid_w_390', 'tfid_w_391', 'tfid_w_392', 'tfid_w_393', 'tfid_w_394', 'tfid_w_395', 'tfid_w_396', 'tfid_w_397', 'tfid_w_398', 'tfid_w_399', 'tfid_w_400', 'tfid_w_401', 'tfid_w_402', 'tfid_w_403', 'tfid_w_404', 'tfid_w_405', 'tfid_w_406', 'tfid_w_407', 'tfid_w_408', 'tfid_w_409', 'tfid_w_410', 'tfid_w_411', 'tfid_w_412', 'tfid_w_413', 'tfid_w_414', 'tfid_w_415', 'tfid_w_416', 'tfid_w_417', 'tfid_w_418', 'tfid_w_419', 'tfid_w_420', 'tfid_w_421', 'tfid_w_422', 'tfid_w_423', 'tfid_w_424', 'tfid_w_425', 'tfid_w_426', 'tfid_w_427', 'tfid_w_428', 'tfid_w_429', 'tfid_w_430', 'tfid_w_431', 'tfid_w_432', 'tfid_w_433', 'tfid_w_434', 'tfid_w_435', 'tfid_w_436', 'tfid_w_437', 'tfid_w_438', 'tfid_w_439', 'tfid_w_440', 'tfid_w_441', 'tfid_w_442', 'tfid_w_443', 'tfid_w_444', 'tfid_w_445', 'tfid_w_446', 'tfid_w_447', 'tfid_w_448', 'tfid_w_449', 'tfid_w_450', 'tfid_w_451', 'tfid_w_452', 'tfid_w_453', 'tfid_w_454', 'tfid_w_455', 'tfid_w_456', 'tfid_w_457', 'tfid_w_458', 'tfid_w_459', 'tfid_w_460', 'tfid_w_461', 'tfid_w_462', 'tfid_w_463', 'tfid_w_464', 'tfid_w_465', 'tfid_w_466', 'tfid_w_467', 'tfid_w_468', 'tfid_w_469', 'tfid_w_470', 'tfid_w_471', 'tfid_w_472', 'tfid_w_473', 'tfid_w_474', 'tfid_w_475', 'tfid_w_476', 'tfid_w_477', 'tfid_w_478', 'tfid_w_479', 'tfid_w_480', 'tfid_w_481', 'tfid_w_482', 'tfid_w_483', 'tfid_w_484', 'tfid_w_485', 'tfid_w_486', 'tfid_w_487', 'tfid_w_488', 'tfid_w_489', 'tfid_w_490', 'tfid_w_491', 'tfid_w_492', 'tfid_w_493', 'tfid_w_494', 'tfid_w_495', 'tfid_w_496', 'tfid_w_497', 'tfid_w_498', 'tfid_w_499', 'tfid_w_500', 'tfid_w_501', 'tfid_w_502', 'tfid_w_503', 'tfid_w_504', 'tfid_w_505', 'tfid_w_506', 'tfid_w_507', 'tfid_w_508', 'tfid_w_509', 'tfid_w_510', 'tfid_w_511', 'tfid_w_512', 'tfid_w_513'] + [target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>paragraph_350_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_w_364</th>\n",
       "      <th>tfid_w_365</th>\n",
       "      <th>tfid_w_366</th>\n",
       "      <th>tfid_w_367</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>splling_err_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2607</td>\n",
       "      <td>492</td>\n",
       "      <td>222</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1630</td>\n",
       "      <td>335</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3009</td>\n",
       "      <td>550</td>\n",
       "      <td>226</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2622</td>\n",
       "      <td>450</td>\n",
       "      <td>213</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2137</td>\n",
       "      <td>372</td>\n",
       "      <td>140</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  paragraph_125_cnt  \\\n",
       "0                 1                 1                  1                  1   \n",
       "1                 5                 5                  5                  5   \n",
       "2                 4                 4                  4                  4   \n",
       "3                 5                 5                  5                  5   \n",
       "4                 4                 4                  4                  4   \n",
       "\n",
       "   paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  paragraph_250_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  4                  3   \n",
       "2                  4                  4                  4                  4   \n",
       "3                  4                  4                  4                  4   \n",
       "4                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_300_cnt  paragraph_350_cnt  ...  tfid_w_364  tfid_w_365  \\\n",
       "0                  1                  1  ...         0.0         0.0   \n",
       "1                  3                  2  ...         0.0         0.0   \n",
       "2                  4                  4  ...         0.0         0.0   \n",
       "3                  4                  4  ...         0.0         0.0   \n",
       "4                  4                  4  ...         0.0         0.0   \n",
       "\n",
       "   tfid_w_366  tfid_w_367  text_length  word_count  unique_word_count  \\\n",
       "0         0.0         0.0         2607         492                222   \n",
       "1         0.0         0.0         1630         335                145   \n",
       "2         0.0         0.0         3009         550                226   \n",
       "3         0.0         0.0         2622         450                213   \n",
       "4         0.0         0.0         2137         372                140   \n",
       "\n",
       "   sentence_count  paragraph_count  splling_err_num  \n",
       "0              14                1               23  \n",
       "1              20                5                7  \n",
       "2              25                4                7  \n",
       "3              24                5                8  \n",
       "4              16                6               14  \n",
       "\n",
       "[5 rows x 3754 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.drop(columns=train_drop_columns).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_drop_columns = [\"essay_id\", \"full_text\", \"paragraph\", \"text_tokens\", \"processed_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>paragraph_350_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tfid_w_364</th>\n",
       "      <th>tfid_w_365</th>\n",
       "      <th>tfid_w_366</th>\n",
       "      <th>tfid_w_367</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>splling_err_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065463</td>\n",
       "      <td>0.065463</td>\n",
       "      <td>2607</td>\n",
       "      <td>492</td>\n",
       "      <td>222</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1630</td>\n",
       "      <td>335</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3009</td>\n",
       "      <td>550</td>\n",
       "      <td>226</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  paragraph_125_cnt  \\\n",
       "0                 1                 1                  1                  1   \n",
       "1                 5                 5                  5                  5   \n",
       "2                 4                 4                  4                  4   \n",
       "\n",
       "   paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  paragraph_250_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  4                  3   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_300_cnt  paragraph_350_cnt  ...  tfid_w_364  tfid_w_365  \\\n",
       "0                  1                  1  ...    0.065463    0.000000   \n",
       "1                  3                  2  ...    0.000000    0.181076   \n",
       "2                  4                  4  ...    0.000000    0.000000   \n",
       "\n",
       "   tfid_w_366  tfid_w_367  text_length  word_count  unique_word_count  \\\n",
       "0    0.065463    0.065463         2607         492                222   \n",
       "1    0.000000    0.000000         1630         335                145   \n",
       "2    0.000000    0.000000         3009         550                226   \n",
       "\n",
       "   sentence_count  paragraph_count  splling_err_num  \n",
       "0              14                1               23  \n",
       "1              20                5                7  \n",
       "2              25                4                7  \n",
       "\n",
       "[3 rows x 3754 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats.drop(columns=test_drop_columns).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Evaluation function and loss function defination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea from https://www.kaggle.com/code/rsakata/optimize-qwk-by-lgb/notebook#QWK-objective\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    y_true = y_true + a\n",
    "    y_pred = (y_pred + a).clip(1, 6).round()\n",
    "    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "    return 'QWK', qwk, True\n",
    "\n",
    "def qwk_obj(y_true, y_pred):\n",
    "    labels = y_true + a\n",
    "    preds = y_pred + a\n",
    "    preds = preds.clip(1, 6)\n",
    "    f = 1/2*np.sum((preds-labels)**2)\n",
    "    g = 1/2*np.sum((preds-a)**2+b)\n",
    "    df = preds - labels\n",
    "    dg = preds - a\n",
    "    grad = (df/g - f*dg/g**2)*len(labels)\n",
    "    hess = np.ones(len(labels))\n",
    "    return grad, hess\n",
    "a = 2.948\n",
    "b = 1.092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Training LGBMRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold_1 Training ================================\n",
      "\n",
      "[LightGBM] [Info] Using self-defined objective function\n",
      "Training until validation scores don't improve for 75 rounds\n",
      "[25]\ttrain's QWK: 0.755278\tvalid's QWK: 0.744537\n",
      "[50]\ttrain's QWK: 0.800207\tvalid's QWK: 0.777743\n",
      "[75]\ttrain's QWK: 0.817787\tvalid's QWK: 0.788816\n",
      "[100]\ttrain's QWK: 0.828686\tvalid's QWK: 0.793632\n",
      "[125]\ttrain's QWK: 0.83737\tvalid's QWK: 0.795988\n",
      "[150]\ttrain's QWK: 0.844961\tvalid's QWK: 0.796691\n",
      "[175]\ttrain's QWK: 0.851675\tvalid's QWK: 0.801721\n",
      "[200]\ttrain's QWK: 0.858156\tvalid's QWK: 0.800466\n",
      "[225]\ttrain's QWK: 0.863782\tvalid's QWK: 0.798881\n",
      "[250]\ttrain's QWK: 0.869761\tvalid's QWK: 0.799522\n",
      "Early stopping, best iteration is:\n",
      "[181]\ttrain's QWK: 0.853508\tvalid's QWK: 0.802415\n",
      "Evaluated only: QWK\n",
      "\n",
      "Fold_2 Training ================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "callbacks = [\n",
    "    lgb.log_evaluation(period=25), \n",
    "    lgb.early_stopping(stopping_rounds=75,first_metric_only=True)\n",
    "]\n",
    "for fold in range(CFG.n_splits):\n",
    "\n",
    "    model = lgb.LGBMRegressor(\n",
    "        objective = qwk_obj, metrics = 'None', learning_rate = 0.1, max_depth = 5,\n",
    "        num_leaves = 10, colsample_bytree=0.5, reg_alpha = 0.1, reg_lambda = 0.8,\n",
    "        n_estimators=1024, random_state=CFG.seed, verbosity = - 1\n",
    "    )\n",
    "    \n",
    "    # Take out the training and validation sets for 5 kfold segmentation separately\n",
    "    X_train = train_feats[train_feats[\"fold\"] != fold].drop(columns=train_drop_columns)\n",
    "    y_train = train_feats[train_feats[\"fold\"] != fold][\"score\"] - a\n",
    "\n",
    "    X_eval = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n",
    "    y_eval = train_feats[train_feats[\"fold\"] == fold][\"score\"] - a\n",
    "\n",
    "    print('\\nFold_{} Training ================================\\n'.format(fold+1))\n",
    "    # Training model\n",
    "    lgb_model = model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_names=['train', 'valid'],\n",
    "        eval_set=[(X_train, y_train), (X_eval, y_eval)],\n",
    "        eval_metric=quadratic_weighted_kappa,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Validating LGBMRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score : 0.8026743411070119\n"
     ]
    }
   ],
   "source": [
    "preds, trues = [], []\n",
    "    \n",
    "for fold, model in enumerate(models):\n",
    "    X_eval_cv = train_feats[train_feats[\"fold\"] == fold].drop(columns=train_drop_columns)\n",
    "    y_eval_cv = train_feats[train_feats[\"fold\"] == fold][\"score\"]\n",
    "\n",
    "    pred = model.predict(X_eval_cv) + a\n",
    "    \n",
    "    trues.extend(y_eval_cv)\n",
    "    preds.extend(np.round(pred, 0))\n",
    "\n",
    "v_score = cohen_kappa_score(trues, preds, weights=\"quadratic\")\n",
    "\n",
    "print(f\"Validation score : {v_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Testing and collecting prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17307, 3907)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3759)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3461, 3754)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_eval_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra features in the model: []\n",
      "Extra features in the input data: []\n"
     ]
    }
   ],
   "source": [
    "# Get the feature columns of the model\n",
    "model_columns = model.feature_name_\n",
    "\n",
    "# Get the feature columns of the input data\n",
    "input_columns = X_eval_cv.columns\n",
    "\n",
    "# Find extra features in the model\n",
    "extra_features_model = [col for col in model_columns if col not in input_columns]\n",
    "\n",
    "# Find extra features in the input data\n",
    "extra_features_input = [col for col in input_columns if col not in model_columns]\n",
    "\n",
    "# Print or inspect the extra features\n",
    "print(\"Extra features in the model:\", extra_features_model)\n",
    "print(\"Extra features in the input data:\", extra_features_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(extra_features_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(extra_features_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecting for 5 models\n",
    "preds = []\n",
    "for fold, model in enumerate(models):\n",
    "    X_eval_cv = test_feats.drop(columns=test_drop_columns)\n",
    "    # pred = model.predict(X_eval_cv)\n",
    "    pred = model.predict(X_eval_cv) + a\n",
    "    preds.append(pred)\n",
    "\n",
    "# Combining the 5 model results\n",
    "for i, pred in enumerate(preds):\n",
    "    test_feats[f\"score_pred_{i}\"] = pred\n",
    "test_feats[\"score\"] = np.round(test_feats[[f\"score_pred_{fold}\" for fold in range(CFG.n_splits)]].mean(axis=1),0).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>paragraph_50_cnt</th>\n",
       "      <th>paragraph_75_cnt</th>\n",
       "      <th>paragraph_100_cnt</th>\n",
       "      <th>paragraph_125_cnt</th>\n",
       "      <th>paragraph_150_cnt</th>\n",
       "      <th>paragraph_175_cnt</th>\n",
       "      <th>paragraph_200_cnt</th>\n",
       "      <th>paragraph_250_cnt</th>\n",
       "      <th>paragraph_300_cnt</th>\n",
       "      <th>...</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>paragraph_count</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>score_pred_0</th>\n",
       "      <th>score_pred_1</th>\n",
       "      <th>score_pred_2</th>\n",
       "      <th>score_pred_3</th>\n",
       "      <th>score_pred_4</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>222</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>2.167996</td>\n",
       "      <td>1.985110</td>\n",
       "      <td>2.203904</td>\n",
       "      <td>1.575921</td>\n",
       "      <td>2.322956</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>145</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>2.789518</td>\n",
       "      <td>2.915989</td>\n",
       "      <td>2.881348</td>\n",
       "      <td>2.908497</td>\n",
       "      <td>2.713750</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>226</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4.754231</td>\n",
       "      <td>4.854331</td>\n",
       "      <td>4.582892</td>\n",
       "      <td>4.602405</td>\n",
       "      <td>4.787978</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 3765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id  paragraph_50_cnt  paragraph_75_cnt  paragraph_100_cnt  \\\n",
       "0  000d118                 1                 1                  1   \n",
       "1  000fe60                 5                 5                  5   \n",
       "2  001ab80                 4                 4                  4   \n",
       "\n",
       "   paragraph_125_cnt  paragraph_150_cnt  paragraph_175_cnt  paragraph_200_cnt  \\\n",
       "0                  1                  1                  1                  1   \n",
       "1                  5                  5                  5                  4   \n",
       "2                  4                  4                  4                  4   \n",
       "\n",
       "   paragraph_250_cnt  paragraph_300_cnt  ...  unique_word_count  \\\n",
       "0                  1                  1  ...                222   \n",
       "1                  3                  3  ...                145   \n",
       "2                  4                  4  ...                226   \n",
       "\n",
       "   sentence_count  paragraph_count  splling_err_num  score_pred_0  \\\n",
       "0              14                1               23      2.167996   \n",
       "1              20                5                7      2.789518   \n",
       "2              25                4                7      4.754231   \n",
       "\n",
       "   score_pred_1  score_pred_2  score_pred_3  score_pred_4  score  \n",
       "0      1.985110      2.203904      1.575921      2.322956      2  \n",
       "1      2.915989      2.881348      2.908497      2.713750      3  \n",
       "2      4.854331      4.582892      4.602405      4.787978      5  \n",
       "\n",
       "[3 rows x 3765 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feats[[\"essay_id\", \"score\"]].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Save Model using Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'lgbm_model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
